# Database Query Optimization for Annotations & Ground Truth

## üéØ Performance Challenge

The current database queries suffer from N+1 query problems, missing indexes, and inefficient data retrieval patterns, leading to 2-5 second response times for annotation and ground truth operations.

## üîç Current Database Performance Issues

### Critical Bottlenecks Identified

#### 1. N+1 Query Problem in Video Retrieval (Lines 826-847)
```python
# CURRENT PROBLEMATIC CODE
query = text("""
    WITH ground_truth_counts AS (
        SELECT 
            video_id,
            COUNT(*) as detection_count
        FROM ground_truth_objects 
        GROUP BY video_id
    )
    SELECT 
        v.id,
        v.filename,
        v.status,
        v.created_at,
        v.duration,
        v.file_size,
        v.ground_truth_generated,
        COALESCE(gtc.detection_count, 0) as detection_count
    FROM videos v
    LEFT JOIN ground_truth_counts gtc ON v.id = gtc.video_id
    WHERE v.project_id = :project_id
    ORDER BY v.created_at DESC
""")
```

**Issue**: While this query uses CTE, it's still inefficient for large datasets and lacks proper indexing.

#### 2. Missing Database Indexes
```sql
-- CURRENT STATE: No performance indexes
CREATE TABLE videos (
    id VARCHAR PRIMARY KEY,
    filename VARCHAR,
    project_id VARCHAR,
    created_at TIMESTAMP,
    -- ‚ö†Ô∏è MISSING: No indexes on frequently queried columns
);

CREATE TABLE ground_truth_objects (
    id VARCHAR PRIMARY KEY,
    video_id VARCHAR,
    frame_number INTEGER,
    -- ‚ö†Ô∏è MISSING: No composite indexes for common queries
);
```

**Impact**: Linear scan operations, slow filtering and sorting.

#### 3. Inefficient Connection Management
```python
# PROBLEMATIC: New connection per request
def get_db():
    db = SessionLocal()  # ‚ö†Ô∏è New connection every time
    try:
        yield db
    finally:
        db.close()
```

**Impact**: Connection overhead, resource exhaustion under load.

## üìä Current Performance Metrics

### Baseline Performance
- **Video List Query**: 500ms-2s (for 1000+ videos)
- **Ground Truth Retrieval**: 1-3s (for video with 10k+ objects)
- **Detection Events Query**: 2-5s (complex joins)
- **Connection Pool**: Not implemented
- **Query Cache Hit Rate**: 0% (no caching)

### Target Performance Goals
- **Video List Query**: <100ms
- **Ground Truth Retrieval**: <200ms
- **Detection Events Query**: <300ms
- **Connection Pool**: 80%+ utilization efficiency
- **Query Cache Hit Rate**: >90%

## üöÄ Database Optimization Strategy

### Phase 1: Index Optimization

#### 1.1 Critical Performance Indexes
```sql
-- Video performance indexes
CREATE INDEX CONCURRENTLY idx_videos_project_id_created_at 
ON videos (project_id, created_at DESC);

CREATE INDEX CONCURRENTLY idx_videos_status_project_id 
ON videos (status, project_id) WHERE status IN ('completed', 'uploaded');

CREATE INDEX CONCURRENTLY idx_videos_ground_truth_generated 
ON videos (ground_truth_generated, project_id) WHERE ground_truth_generated = true;

-- Ground truth performance indexes
CREATE INDEX CONCURRENTLY idx_ground_truth_video_id_frame 
ON ground_truth_objects (video_id, frame_number);

CREATE INDEX CONCURRENTLY idx_ground_truth_class_confidence 
ON ground_truth_objects (class_label, confidence) WHERE confidence > 0.5;

CREATE INDEX CONCURRENTLY idx_ground_truth_timestamp 
ON ground_truth_objects (video_id, timestamp);

-- Detection events performance indexes
CREATE INDEX CONCURRENTLY idx_detection_events_session_timestamp 
ON detection_events (test_session_id, timestamp);

CREATE INDEX CONCURRENTLY idx_detection_events_video_confidence 
ON detection_events (test_session_id, confidence DESC) WHERE confidence > 0.3;

CREATE INDEX CONCURRENTLY idx_detection_events_class_validation 
ON detection_events (class_label, validation_result);

-- Test sessions performance indexes
CREATE INDEX CONCURRENTLY idx_test_sessions_project_status 
ON test_sessions (project_id, status, started_at DESC);

CREATE INDEX CONCURRENTLY idx_test_sessions_video_id 
ON test_sessions (video_id, status) WHERE status = 'completed';
```

#### 1.2 Composite Indexes for Complex Queries
```sql
-- Multi-column indexes for common filter combinations
CREATE INDEX CONCURRENTLY idx_videos_project_status_date 
ON videos (project_id, status, created_at DESC) 
WHERE status IN ('completed', 'uploaded', 'processing');

CREATE INDEX CONCURRENTLY idx_ground_truth_video_class_frame 
ON ground_truth_objects (video_id, class_label, frame_number) 
WHERE validated = true;

CREATE INDEX CONCURRENTLY idx_detection_events_comprehensive 
ON detection_events (test_session_id, class_label, confidence DESC, timestamp) 
WHERE validation_result != 'PENDING';
```

### Phase 2: Query Optimization

#### 2.1 Optimized Video Retrieval with Pagination
```python
class OptimizedVideoQueries:
    def __init__(self, db: Session):
        self.db = db
        
    def get_project_videos_optimized(
        self, 
        project_id: str, 
        offset: int = 0, 
        limit: int = 50,
        status_filter: Optional[str] = None
    ) -> Dict[str, Any]:
        """Highly optimized video retrieval with pagination"""
        
        # Base query with optimal joins and filtering
        base_query = """
        WITH video_stats AS (
            SELECT 
                v.id,
                v.filename,
                v.status,
                v.created_at,
                v.duration,
                v.file_size,
                v.ground_truth_generated,
                v.processing_status,
                -- Optimized aggregation using window functions
                COUNT(gto.id) OVER (PARTITION BY v.id) as detection_count,
                MAX(gto.confidence) OVER (PARTITION BY v.id) as max_confidence,
                -- Row numbering for efficient pagination
                ROW_NUMBER() OVER (ORDER BY v.created_at DESC) as rn
            FROM videos v
            LEFT JOIN ground_truth_objects gto ON v.id = gto.video_id 
                AND gto.confidence > 0.3  -- Filter low confidence early
            WHERE v.project_id = :project_id
        """
        
        # Add status filter if provided
        if status_filter:
            base_query += " AND v.status = :status_filter"
        
        # Complete query with pagination
        query = f"""
        {base_query}
        )
        SELECT DISTINCT
            id, filename, status, created_at, duration, 
            file_size, ground_truth_generated, processing_status,
            detection_count, max_confidence
        FROM video_stats 
        WHERE rn BETWEEN :offset + 1 AND :offset + :limit
        ORDER BY created_at DESC
        """
        
        # Execute with proper parameter binding
        params = {
            'project_id': project_id,
            'offset': offset,
            'limit': limit
        }
        if status_filter:
            params['status_filter'] = status_filter
            
        result = self.db.execute(text(query), params)
        videos = result.fetchall()
        
        # Get total count efficiently
        count_query = """
        SELECT COUNT(*) 
        FROM videos 
        WHERE project_id = :project_id
        """ + (" AND status = :status_filter" if status_filter else "")
        
        total_count = self.db.execute(text(count_query), params).scalar()
        
        return {
            'videos': [self._video_row_to_dict(video) for video in videos],
            'total_count': total_count,
            'has_more': offset + limit < total_count
        }
    
    def get_ground_truth_optimized(
        self, 
        video_id: str, 
        min_confidence: float = 0.3
    ) -> Dict[str, Any]:
        """Optimized ground truth retrieval with filtering"""
        
        query = """
        SELECT 
            gto.id,
            gto.frame_number,
            gto.timestamp,
            gto.class_label,
            gto.confidence,
            gto.x, gto.y, gto.width, gto.height,
            gto.validated,
            gto.difficult,
            -- Pre-calculate bounding box area
            (gto.width * gto.height) as bbox_area
        FROM ground_truth_objects gto
        WHERE gto.video_id = :video_id
            AND gto.confidence >= :min_confidence
        ORDER BY gto.frame_number, gto.confidence DESC
        """
        
        result = self.db.execute(text(query), {
            'video_id': video_id,
            'min_confidence': min_confidence
        })
        
        objects = result.fetchall()
        
        # Aggregate statistics
        stats_query = """
        SELECT 
            COUNT(*) as total_objects,
            AVG(confidence) as avg_confidence,
            COUNT(DISTINCT class_label) as unique_classes,
            COUNT(CASE WHEN validated THEN 1 END) as validated_count
        FROM ground_truth_objects
        WHERE video_id = :video_id AND confidence >= :min_confidence
        """
        
        stats = self.db.execute(text(stats_query), {
            'video_id': video_id,
            'min_confidence': min_confidence
        }).fetchone()
        
        return {
            'video_id': video_id,
            'objects': [self._ground_truth_row_to_dict(obj) for obj in objects],
            'statistics': {
                'total_objects': stats.total_objects,
                'avg_confidence': float(stats.avg_confidence or 0),
                'unique_classes': stats.unique_classes,
                'validated_count': stats.validated_count,
                'validation_rate': (stats.validated_count / stats.total_objects * 100) 
                                 if stats.total_objects > 0 else 0
            }
        }
```

#### 2.2 Batch Operations for Bulk Updates
```python
class BatchDatabaseOperations:
    def __init__(self, db: Session):
        self.db = db
        
    def bulk_insert_detections(self, detections: List[Dict]) -> int:
        """Optimized bulk insertion of detection events"""
        if not detections:
            return 0
            
        # Use SQLAlchemy Core for bulk operations (faster than ORM)
        from sqlalchemy import insert
        
        # Prepare data for bulk insert
        insert_data = []
        for detection in detections:
            insert_data.append({
                'id': detection['id'],
                'test_session_id': detection['test_session_id'],
                'timestamp': detection['timestamp'],
                'confidence': detection['confidence'],
                'class_label': detection['class_label'],
                'validation_result': detection.get('validation_result', 'PENDING'),
                'frame_number': detection.get('frame_number'),
                'bounding_box_x': detection.get('bounding_box', {}).get('x'),
                'bounding_box_y': detection.get('bounding_box', {}).get('y'),
                'bounding_box_width': detection.get('bounding_box', {}).get('width'),
                'bounding_box_height': detection.get('bounding_box', {}).get('height'),
                'created_at': datetime.utcnow()
            })
        
        # Bulk insert with ON CONFLICT handling
        stmt = insert(DetectionEvent.__table__)
        stmt = stmt.on_conflict_do_update(
            index_elements=['id'],
            set_=dict(
                confidence=stmt.excluded.confidence,
                validation_result=stmt.excluded.validation_result,
                updated_at=datetime.utcnow()
            )
        )
        
        self.db.execute(stmt, insert_data)
        self.db.commit()
        
        return len(insert_data)
    
    def bulk_update_validation_results(
        self, 
        detection_ids: List[str], 
        validation_result: str
    ) -> int:
        """Bulk update validation results"""
        if not detection_ids:
            return 0
            
        # Batch update using raw SQL for performance
        update_query = """
        UPDATE detection_events 
        SET validation_result = :validation_result,
            updated_at = :updated_at
        WHERE id = ANY(:detection_ids)
        """
        
        result = self.db.execute(text(update_query), {
            'validation_result': validation_result,
            'updated_at': datetime.utcnow(),
            'detection_ids': detection_ids
        })
        
        self.db.commit()
        return result.rowcount
```

### Phase 3: Connection Pool Optimization

#### 3.1 Advanced Connection Pool Configuration
```python
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool
from sqlalchemy.orm import sessionmaker
import logging

# Configure logging for connection pool monitoring
logging.getLogger('sqlalchemy.pool').setLevel(logging.INFO)

class DatabaseConfig:
    def __init__(self):
        self.DATABASE_URL = "postgresql://user:password@localhost/dbname"
        
        # Optimized connection pool settings
        self.engine = create_engine(
            self.DATABASE_URL,
            
            # Connection pool configuration
            poolclass=QueuePool,
            pool_size=20,          # Base number of connections
            max_overflow=30,       # Additional connections under load
            pool_pre_ping=True,    # Validate connections before use
            pool_recycle=3600,     # Recycle connections every hour
            pool_reset_on_return='commit',  # Clean state on return
            
            # Query optimization
            echo=False,            # Disable SQL logging in production
            echo_pool=True,        # Enable pool logging for monitoring
            
            # Connection-level optimizations
            connect_args={
                "application_name": "ai_validation_platform",
                "options": "-c default_transaction_isolation=read_committed "
                          "-c statement_timeout=30s "
                          "-c idle_in_transaction_session_timeout=60s"
            }
        )
        
        self.SessionLocal = sessionmaker(
            bind=self.engine,
            autoflush=False,       # Manual flush control
            autocommit=False,      # Explicit transaction control
            expire_on_commit=False # Keep objects accessible after commit
        )

# Connection pool monitoring
class ConnectionPoolMonitor:
    def __init__(self, engine):
        self.engine = engine
        
    def get_pool_status(self) -> Dict[str, Any]:
        pool = self.engine.pool
        return {
            'pool_size': pool.size(),
            'checked_in': pool.checkedin(),
            'checked_out': pool.checkedout(),
            'overflow': pool.overflow(),
            'invalid': pool.invalid(),
            'pool_usage': (pool.checkedout() / (pool.size() + pool.overflow())) * 100
        }
    
    def log_pool_stats(self):
        stats = self.get_pool_status()
        logger.info(f"Connection Pool Stats: {stats}")
        
        # Alert if pool utilization is high
        if stats['pool_usage'] > 80:
            logger.warning(f"High connection pool utilization: {stats['pool_usage']:.1f}%")
```

#### 3.2 Optimized Database Dependency
```python
from contextlib import asynccontextmanager
from functools import lru_cache

class DatabaseManager:
    def __init__(self):
        self.db_config = DatabaseConfig()
        self.pool_monitor = ConnectionPoolMonitor(self.db_config.engine)
        
    @asynccontextmanager
    async def get_db_context(self):
        """Async context manager for database sessions"""
        db = self.db_config.SessionLocal()
        try:
            yield db
            db.commit()
        except Exception as e:
            db.rollback()
            logger.error(f"Database transaction failed: {str(e)}")
            raise
        finally:
            db.close()
    
    def get_db_session(self):
        """Standard database dependency for FastAPI"""
        db = self.db_config.SessionLocal()
        try:
            yield db
        except SQLAlchemyError as e:
            db.rollback()
            logger.error(f"Database error: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail="Database operation failed"
            )
        finally:
            db.close()

# Global database manager instance
db_manager = DatabaseManager()

# FastAPI dependency
def get_db():
    yield from db_manager.get_db_session()

# Health check endpoint for database
@app.get("/health/database")
async def database_health_check():
    try:
        async with db_manager.get_db_context() as db:
            # Simple connectivity test
            result = db.execute(text("SELECT 1 as health_check"))
            health_check = result.scalar()
            
            # Pool statistics
            pool_stats = db_manager.pool_monitor.get_pool_status()
            
            return {
                "status": "healthy" if health_check == 1 else "unhealthy",
                "connection_pool": pool_stats,
                "timestamp": datetime.utcnow().isoformat()
            }
    except Exception as e:
        logger.error(f"Database health check failed: {str(e)}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.utcnow().isoformat()
        }
```

### Phase 4: Query Caching Strategy

#### 4.1 Redis-Based Query Caching
```python
import redis
import json
import hashlib
from typing import Optional, Any

class QueryCache:
    def __init__(self, redis_url: str = "redis://localhost:6379/0"):
        self.redis_client = redis.from_url(redis_url)
        self.default_ttl = 3600  # 1 hour default TTL
        
    def _generate_cache_key(self, query: str, params: Dict) -> str:
        """Generate unique cache key for query and parameters"""
        cache_data = {
            'query': query,
            'params': sorted(params.items()) if params else []
        }
        cache_string = json.dumps(cache_data, sort_keys=True)
        return hashlib.sha256(cache_string.encode()).hexdigest()
    
    def get_cached_result(self, query: str, params: Dict = None) -> Optional[Any]:
        """Retrieve cached query result"""
        cache_key = self._generate_cache_key(query, params or {})
        
        try:
            cached_data = self.redis_client.get(cache_key)
            if cached_data:
                return json.loads(cached_data)
        except Exception as e:
            logger.error(f"Cache retrieval error: {str(e)}")
        
        return None
    
    def cache_result(
        self, 
        query: str, 
        params: Dict, 
        result: Any, 
        ttl: Optional[int] = None
    ):
        """Cache query result with TTL"""
        cache_key = self._generate_cache_key(query, params)
        ttl = ttl or self.default_ttl
        
        try:
            # Serialize result to JSON
            cached_data = json.dumps(result, default=str)  # default=str for datetime objects
            self.redis_client.setex(cache_key, ttl, cached_data)
        except Exception as e:
            logger.error(f"Cache storage error: {str(e)}")
    
    def invalidate_pattern(self, pattern: str):
        """Invalidate cache entries matching pattern"""
        try:
            keys = self.redis_client.keys(pattern)
            if keys:
                self.redis_client.delete(*keys)
                logger.info(f"Invalidated {len(keys)} cache entries")
        except Exception as e:
            logger.error(f"Cache invalidation error: {str(e)}")

# Cache-aware query decorator
def cached_query(ttl: int = 3600, cache_pattern: str = None):
    """Decorator for caching database query results"""
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            query_cache = QueryCache()
            
            # Generate cache key from function name and parameters
            cache_key_data = {
                'function': func.__name__,
                'args': args,
                'kwargs': kwargs
            }
            
            # Try to get from cache first
            cached_result = query_cache.get_cached_result(
                func.__name__, 
                cache_key_data
            )
            
            if cached_result is not None:
                logger.debug(f"Cache hit for {func.__name__}")
                return cached_result
            
            # Execute actual query
            logger.debug(f"Cache miss for {func.__name__}, executing query")
            result = func(*args, **kwargs)
            
            # Cache the result
            query_cache.cache_result(
                func.__name__, 
                cache_key_data, 
                result, 
                ttl
            )
            
            return result
        return wrapper
    return decorator
```

#### 4.2 Intelligent Cache Invalidation
```python
class SmartCacheInvalidation:
    def __init__(self, query_cache: QueryCache):
        self.cache = query_cache
        
    def invalidate_video_cache(self, project_id: str, video_id: str = None):
        """Invalidate video-related cache entries"""
        patterns = [
            f"get_project_videos_optimized:*{project_id}*",
            f"get_enhanced_dashboard_stats:*",
        ]
        
        if video_id:
            patterns.extend([
                f"get_ground_truth_optimized:*{video_id}*",
                f"get_video_detections:*{video_id}*"
            ])
        
        for pattern in patterns:
            self.cache.invalidate_pattern(pattern)
    
    def invalidate_detection_cache(self, test_session_id: str):
        """Invalidate detection-related cache entries"""
        patterns = [
            f"get_test_session_detections:*{test_session_id}*",
            f"get_test_results:*{test_session_id}*",
            f"get_dashboard_stats:*"
        ]
        
        for pattern in patterns:
            self.cache.invalidate_pattern(pattern)

# Database event listeners for automatic cache invalidation
from sqlalchemy import event

@event.listens_for(Video, 'after_insert')
@event.listens_for(Video, 'after_update')
@event.listens_for(Video, 'after_delete')
def invalidate_video_cache_on_change(mapper, connection, target):
    """Automatically invalidate cache when videos change"""
    cache_invalidator = SmartCacheInvalidation(QueryCache())
    cache_invalidator.invalidate_video_cache(target.project_id, target.id)

@event.listens_for(DetectionEvent, 'after_insert')
@event.listens_for(DetectionEvent, 'after_update')
def invalidate_detection_cache_on_change(mapper, connection, target):
    """Automatically invalidate cache when detections change"""
    cache_invalidator = SmartCacheInvalidation(QueryCache())
    cache_invalidator.invalidate_detection_cache(target.test_session_id)
```

## üìà Expected Performance Improvements

### Query Performance Benchmarks

| Query Type | Current | Phase 1 | Phase 2 | Phase 3 | Phase 4 | Improvement |
|------------|---------|---------|---------|---------|---------|-------------|
| Video List (1K videos) | 2000ms | 300ms | 150ms | 100ms | 20ms | **100x faster** |
| Ground Truth (10K objects) | 3000ms | 500ms | 250ms | 200ms | 50ms | **60x faster** |
| Detection Events | 5000ms | 800ms | 400ms | 300ms | 80ms | **62x faster** |
| Dashboard Stats | 2000ms | 400ms | 200ms | 100ms | 10ms | **200x faster** |
| Bulk Operations | 10000ms | 2000ms | 1000ms | 500ms | 200ms | **50x faster** |

### Connection Pool Benefits
- **Connection Overhead**: Reduced by 85%
- **Resource Utilization**: 80%+ pool efficiency
- **Concurrent Requests**: Support for 100+ simultaneous queries
- **Memory Usage**: 60% reduction in database connections

## üîß Implementation Roadmap

### Week 1: Index Implementation
- [ ] Create performance indexes with CONCURRENTLY
- [ ] Analyze query execution plans
- [ ] Implement index monitoring queries
- [ ] Validate index effectiveness

### Week 2: Query Optimization
- [ ] Implement optimized query classes
- [ ] Add pagination and filtering
- [ ] Create batch operation methods
- [ ] Performance test query improvements

### Week 3: Connection Pool Setup
- [ ] Configure advanced connection pooling
- [ ] Implement pool monitoring
- [ ] Add health check endpoints
- [ ] Load test connection handling

### Week 4: Caching Integration
- [ ] Setup Redis caching layer
- [ ] Implement cache decorators
- [ ] Add intelligent cache invalidation
- [ ] Monitor cache hit rates

## üõ† Production Deployment

### Database Migration Script
```sql
-- migration.sql
-- Phase 1: Create indexes (can be run online)
\timing on

-- Video indexes
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_videos_project_id_created_at 
ON videos (project_id, created_at DESC);

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_videos_status_project_id 
ON videos (status, project_id) WHERE status IN ('completed', 'uploaded');

-- Ground truth indexes  
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_ground_truth_video_id_frame 
ON ground_truth_objects (video_id, frame_number);

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_ground_truth_class_confidence 
ON ground_truth_objects (class_label, confidence) WHERE confidence > 0.5;

-- Detection event indexes
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_detection_events_session_timestamp 
ON detection_events (test_session_id, timestamp);

-- Analyze tables after index creation
ANALYZE videos;
ANALYZE ground_truth_objects;
ANALYZE detection_events;
ANALYZE test_sessions;

\timing off
```

### Monitoring Setup
```python
# Database performance monitoring
class DatabasePerformanceMonitor:
    def __init__(self, db: Session):
        self.db = db
    
    def get_slow_queries(self, threshold_ms: int = 1000) -> List[Dict]:
        """Get queries slower than threshold"""
        query = """
        SELECT 
            query,
            mean_time,
            calls,
            total_time,
            rows,
            100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
        FROM pg_stat_statements 
        WHERE mean_time > :threshold
        ORDER BY mean_time DESC 
        LIMIT 10
        """
        
        result = self.db.execute(text(query), {'threshold': threshold_ms})
        return [dict(row) for row in result.fetchall()]
    
    def get_index_usage(self) -> List[Dict]:
        """Get index usage statistics"""
        query = """
        SELECT 
            schemaname,
            tablename,
            indexname,
            idx_tup_read,
            idx_tup_fetch,
            idx_scan
        FROM pg_stat_user_indexes 
        ORDER BY idx_scan DESC
        """
        
        result = self.db.execute(text(query))
        return [dict(row) for row in result.fetchall()]
```

## ‚úÖ Success Validation

### Performance Testing
```python
import time
import asyncio

async def performance_test_suite():
    """Comprehensive database performance test"""
    db_manager = DatabaseManager()
    
    tests = [
        ("Video List Query", test_video_list_performance),
        ("Ground Truth Query", test_ground_truth_performance),  
        ("Detection Events Query", test_detection_events_performance),
        ("Bulk Insert Operations", test_bulk_insert_performance),
        ("Cache Hit Rate", test_cache_performance)
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        start_time = time.time()
        result = await test_func(db_manager)
        end_time = time.time()
        
        results[test_name] = {
            'duration': end_time - start_time,
            'result': result,
            'status': 'PASS' if end_time - start_time < 0.5 else 'FAIL'
        }
    
    return results

# Run performance tests
test_results = asyncio.run(performance_test_suite())
print("Database Performance Test Results:", test_results)
```

---

**Priority**: üü° **HIGH - Critical for Scalability**  
**Implementation Time**: 4 weeks  
**Expected Impact**: 20-100x query performance improvement  
**Risk Level**: Medium (requires careful migration)  
**Dependencies**: Redis, PostgreSQL optimization, connection pooling