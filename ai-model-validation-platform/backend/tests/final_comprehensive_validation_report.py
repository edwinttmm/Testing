#!/usr/bin/env python3
"""
FINAL COMPREHENSIVE VALIDATION REPORT
AI Model Validation Platform - Complete Production Readiness Assessment

This script generates a comprehensive final validation report based on all testing
performed by various test engineers, addressing the specific issues found and
providing a definitive production readiness assessment.
"""

import json
import requests
import time
from datetime import datetime
from typing import Dict, Any, List
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class FinalValidationReport:
    """Generate comprehensive final validation report"""
    
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
        self.frontend_url = "http://localhost:3000"
        self.session = requests.Session()
        self.session.timeout = 30
        
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """Generate final comprehensive validation report"""
        
        logger.info("üéØ GENERATING FINAL COMPREHENSIVE VALIDATION REPORT")
        logger.info("=" * 80)
        
        report = {
            "report_metadata": {
                "report_date": datetime.now().isoformat(),
                "report_type": "Final Production Readiness Assessment",
                "environment": "Local Development",
                "backend_url": self.base_url,
                "frontend_url": self.frontend_url
            },
            "executive_summary": {},
            "critical_functionality_assessment": {},
            "performance_assessment": {},
            "error_handling_assessment": {},
            "user_experience_assessment": {},
            "identified_issues": [],
            "system_completeness": {},
            "production_readiness": {},
            "recommendations": []
        }
        
        # Perform comprehensive assessment
        report["critical_functionality_assessment"] = self._assess_critical_functionality()
        report["performance_assessment"] = self._assess_performance()
        report["error_handling_assessment"] = self._assess_error_handling()
        report["user_experience_assessment"] = self._assess_user_experience()
        report["system_completeness"] = self._assess_system_completeness()
        
        # Generate final scores and recommendations
        report["production_readiness"] = self._calculate_production_readiness(report)
        report["executive_summary"] = self._generate_executive_summary(report)
        report["recommendations"] = self._generate_recommendations(report)
        
        return report
    
    def _assess_critical_functionality(self) -> Dict[str, Any]:
        """Assess critical functionality based on actual testing"""
        logger.info("üîç Assessing Critical Functionality...")
        
        assessment = {
            "overall_score": 0,
            "category_scores": {},
            "tests_performed": [],
            "issues_found": []
        }
        
        # Test 1: Video Upload and Management
        video_score = self._test_video_functionality()
        assessment["category_scores"]["video_management"] = video_score
        assessment["tests_performed"].append("Video Upload and Management")
        
        if video_score["score"] < 100:
            assessment["issues_found"].extend(video_score["issues"])
        
        # Test 2: Project Management
        project_score = self._test_project_functionality()
        assessment["category_scores"]["project_management"] = project_score
        assessment["tests_performed"].append("Project Management")
        
        if project_score["score"] < 100:
            assessment["issues_found"].extend(project_score["issues"])
        
        # Test 3: API Endpoints
        api_score = self._test_api_completeness()
        assessment["category_scores"]["api_completeness"] = api_score
        assessment["tests_performed"].append("API Completeness")
        
        if api_score["score"] < 100:
            assessment["issues_found"].extend(api_score["issues"])
        
        # Calculate overall score
        scores = [score["score"] for score in assessment["category_scores"].values()]
        assessment["overall_score"] = sum(scores) / len(scores) if scores else 0
        
        return assessment
    
    def _test_video_functionality(self) -> Dict[str, Any]:
        """Test video functionality comprehensively"""
        issues = []
        
        try:
            # Test video listing
            response = self.session.get(f"{self.base_url}/api/videos")
            if response.status_code != 200:
                issues.append({
                    "severity": "HIGH",
                    "component": "Video Management",
                    "description": f"Video listing endpoint failed: {response.status_code}"
                })
            
            # Test video upload functionality
            import tempfile
            import os
            
            with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as temp_video:
                mp4_content = b'\x00\x00\x00\x20ftypiso m\x00\x00\x02\x00iso miso2avc1mp41' + b'\x00' * 1000
                temp_video.write(mp4_content)
                temp_video_path = temp_video.name
            
            try:
                with open(temp_video_path, 'rb') as f:
                    files = {'file': ('test_validation.mp4', f, 'video/mp4')}
                    upload_response = self.session.post(f"{self.base_url}/api/videos", files=files)
                
                if upload_response.status_code not in [200, 201]:
                    issues.append({
                        "severity": "HIGH",
                        "component": "Video Management", 
                        "description": f"Video upload failed: {upload_response.status_code}"
                    })
                else:
                    upload_data = upload_response.json()
                    if not upload_data.get("id"):
                        issues.append({
                            "severity": "HIGH",
                            "component": "Video Management",
                            "description": "Video upload response missing ID field"
                        })
                        
            finally:
                os.unlink(temp_video_path)
            
            # Note: Individual video retrieval endpoint missing (this is expected based on our findings)
            issues.append({
                "severity": "MEDIUM",
                "component": "Video Management",
                "description": "Individual video retrieval endpoint (/api/videos/{id}) not implemented - affects video workflow completeness"
            })
            
        except Exception as e:
            issues.append({
                "severity": "HIGH",
                "component": "Video Management",
                "description": f"Video functionality test failed: {str(e)}"
            })
        
        # Calculate score based on issues
        critical_issues = len([i for i in issues if i["severity"] == "HIGH"])
        medium_issues = len([i for i in issues if i["severity"] == "MEDIUM"])
        
        if critical_issues == 0 and medium_issues <= 1:
            score = 85  # Good score with minor missing endpoint
        elif critical_issues == 0:
            score = 70
        else:
            score = max(0, 100 - (critical_issues * 30) - (medium_issues * 10))
        
        return {
            "score": score,
            "issues": issues,
            "details": f"Video functionality assessment: {len(issues)} issues found"
        }
    
    def _test_project_functionality(self) -> Dict[str, Any]:
        """Test project management functionality"""
        issues = []
        
        try:
            # Test project listing
            response = self.session.get(f"{self.base_url}/api/projects")
            if response.status_code != 200:
                issues.append({
                    "severity": "HIGH",
                    "component": "Project Management",
                    "description": f"Project listing failed: {response.status_code}"
                })
            
            # Test project creation
            project_data = {
                "name": f"Validation Test Project {datetime.now().strftime('%H%M%S')}",
                "description": "Final validation test project",
                "camera_model": "Test Camera",
                "camera_view": "Front-facing VRU",
                "signal_type": "GPIO"
            }
            
            create_response = self.session.post(f"{self.base_url}/api/projects", json=project_data)
            if create_response.status_code not in [200, 201]:
                issues.append({
                    "severity": "HIGH",
                    "component": "Project Management",
                    "description": f"Project creation failed: {create_response.status_code}"
                })
            else:
                project = create_response.json()
                project_id = project.get("id")
                
                if project_id:
                    # Test project retrieval
                    get_response = self.session.get(f"{self.base_url}/api/projects/{project_id}")
                    if get_response.status_code != 200:
                        issues.append({
                            "severity": "MEDIUM",
                            "component": "Project Management",
                            "description": f"Project retrieval failed: {get_response.status_code}"
                        })
            
        except Exception as e:
            issues.append({
                "severity": "HIGH",
                "component": "Project Management",
                "description": f"Project functionality test failed: {str(e)}"
            })
        
        # Calculate score
        critical_issues = len([i for i in issues if i["severity"] == "HIGH"])
        medium_issues = len([i for i in issues if i["severity"] == "MEDIUM"])
        
        score = max(0, 100 - (critical_issues * 30) - (medium_issues * 10))
        
        return {
            "score": score,
            "issues": issues,
            "details": f"Project functionality assessment: {len(issues)} issues found"
        }
    
    def _test_api_completeness(self) -> Dict[str, Any]:
        """Test API completeness and design consistency"""
        issues = []
        
        try:
            # Check API documentation
            docs_response = self.session.get(f"{self.base_url}/docs")
            if docs_response.status_code != 200:
                issues.append({
                    "severity": "HIGH",
                    "component": "API Documentation",
                    "description": "API documentation not accessible"
                })
            
            # Check health endpoint
            health_response = self.session.get(f"{self.base_url}/health")
            if health_response.status_code != 200:
                issues.append({
                    "severity": "HIGH",
                    "component": "API Health",
                    "description": "Health endpoint not working"
                })
            
            # Known missing endpoint (based on our analysis)
            issues.append({
                "severity": "MEDIUM",
                "component": "API Completeness",
                "description": "Individual video retrieval endpoint missing (/api/videos/{id})"
            })
            
            # Test error handling consistency
            error_response = self.session.get(f"{self.base_url}/api/nonexistent")
            if error_response.status_code != 404:
                issues.append({
                    "severity": "LOW",
                    "component": "API Error Handling",
                    "description": f"Unexpected status code for invalid endpoint: {error_response.status_code}"
                })
            
        except Exception as e:
            issues.append({
                "severity": "HIGH",
                "component": "API Testing",
                "description": f"API completeness test failed: {str(e)}"
            })
        
        # Calculate score
        critical_issues = len([i for i in issues if i["severity"] == "HIGH"])
        medium_issues = len([i for i in issues if i["severity"] == "MEDIUM"])
        low_issues = len([i for i in issues if i["severity"] == "LOW"])
        
        score = max(0, 100 - (critical_issues * 25) - (medium_issues * 10) - (low_issues * 5))
        
        return {
            "score": score,
            "issues": issues,
            "details": f"API completeness assessment: {len(issues)} issues found"
        }
    
    def _assess_performance(self) -> Dict[str, Any]:
        """Assess system performance"""
        logger.info("‚ö° Assessing Performance...")
        
        assessment = {
            "overall_score": 0,
            "response_times": {},
            "load_handling": {},
            "issues_found": []
        }
        
        try:
            # Test response times
            endpoints_to_test = [
                ("/health", "Health Check"),
                ("/api/projects", "Project Listing"),
                ("/api/videos", "Video Listing")
            ]
            
            total_response_time = 0
            successful_tests = 0
            
            for endpoint, name in endpoints_to_test:
                start_time = time.time()
                try:
                    response = self.session.get(f"{self.base_url}{endpoint}")
                    response_time = time.time() - start_time
                    
                    assessment["response_times"][name] = {
                        "time": response_time,
                        "status": response.status_code
                    }
                    
                    if response.status_code == 200:
                        total_response_time += response_time
                        successful_tests += 1
                        
                        if response_time > 5.0:
                            assessment["issues_found"].append({
                                "severity": "MEDIUM",
                                "component": "Performance",
                                "description": f"{name} slow response time: {response_time:.2f}s"
                            })
                        
                except Exception as e:
                    assessment["response_times"][name] = {
                        "error": str(e)
                    }
                    assessment["issues_found"].append({
                        "severity": "HIGH",
                        "component": "Performance",
                        "description": f"{name} performance test failed: {str(e)}"
                    })
            
            # Calculate average response time
            avg_response_time = total_response_time / successful_tests if successful_tests > 0 else 10
            
            # Test concurrent load (simplified)
            start_time = time.time()
            concurrent_requests = []
            
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                futures = [executor.submit(lambda: self.session.get(f"{self.base_url}/health")) for _ in range(10)]
                concurrent_requests = [future.result() for future in concurrent.futures.as_completed(futures)]
            
            load_test_time = time.time() - start_time
            successful_concurrent = sum(1 for r in concurrent_requests if r.status_code == 200)
            
            assessment["load_handling"] = {
                "concurrent_requests": 10,
                "successful_requests": successful_concurrent,
                "total_time": load_test_time,
                "success_rate": (successful_concurrent / 10) * 100
            }
            
            if successful_concurrent < 9:  # Less than 90% success
                assessment["issues_found"].append({
                    "severity": "MEDIUM",
                    "component": "Load Handling",
                    "description": f"Load test success rate too low: {(successful_concurrent/10)*100}%"
                })
            
            # Calculate performance score
            performance_factors = []
            
            # Response time factor (excellent < 1s, good < 3s, poor >= 3s)
            if avg_response_time < 1.0:
                performance_factors.append(100)
            elif avg_response_time < 3.0:
                performance_factors.append(80)
            else:
                performance_factors.append(max(0, 100 - (avg_response_time * 10)))
            
            # Load handling factor
            load_factor = (successful_concurrent / 10) * 100
            performance_factors.append(load_factor)
            
            assessment["overall_score"] = sum(performance_factors) / len(performance_factors)
            
        except Exception as e:
            assessment["issues_found"].append({
                "severity": "HIGH",
                "component": "Performance Assessment",
                "description": f"Performance assessment failed: {str(e)}"
            })
            assessment["overall_score"] = 0
        
        return assessment
    
    def _assess_error_handling(self) -> Dict[str, Any]:
        """Assess error handling capabilities"""
        logger.info("üö® Assessing Error Handling...")
        
        assessment = {
            "overall_score": 0,
            "error_scenarios_tested": [],
            "issues_found": []
        }
        
        error_test_cases = [
            {
                "name": "Invalid Endpoint (404)",
                "method": "GET",
                "url": f"{self.base_url}/api/invalid-endpoint",
                "expected_status": 404
            },
            {
                "name": "Malformed Request Body",
                "method": "POST",
                "url": f"{self.base_url}/api/projects",
                "data": "invalid json",
                "headers": {"Content-Type": "application/json"},
                "expected_status": [400, 422]
            },
            {
                "name": "Missing Required Fields",
                "method": "POST",
                "url": f"{self.base_url}/api/projects",
                "json": {},
                "expected_status": 422
            }
        ]
        
        successful_error_tests = 0
        
        for test_case in error_test_cases:
            try:
                if test_case["method"] == "GET":
                    response = self.session.get(test_case["url"])
                elif test_case["method"] == "POST":
                    if "json" in test_case:
                        response = self.session.post(test_case["url"], json=test_case["json"])
                    elif "data" in test_case:
                        headers = test_case.get("headers", {})
                        response = self.session.post(test_case["url"], data=test_case["data"], headers=headers)
                
                expected = test_case["expected_status"]
                if isinstance(expected, list):
                    status_correct = response.status_code in expected
                else:
                    status_correct = response.status_code == expected
                
                if status_correct:
                    successful_error_tests += 1
                else:
                    assessment["issues_found"].append({
                        "severity": "MEDIUM",
                        "component": "Error Handling",
                        "description": f"{test_case['name']}: Expected {expected}, got {response.status_code}"
                    })
                
                assessment["error_scenarios_tested"].append({
                    "test": test_case["name"],
                    "expected": expected,
                    "actual": response.status_code,
                    "passed": status_correct
                })
                
            except Exception as e:
                assessment["issues_found"].append({
                    "severity": "HIGH",
                    "component": "Error Handling",
                    "description": f"{test_case['name']} test failed: {str(e)}"
                })
        
        assessment["overall_score"] = (successful_error_tests / len(error_test_cases)) * 100
        
        return assessment
    
    def _assess_user_experience(self) -> Dict[str, Any]:
        """Assess user experience factors"""
        logger.info("üë§ Assessing User Experience...")
        
        assessment = {
            "overall_score": 0,
            "frontend_accessibility": {},
            "documentation_quality": {},
            "issues_found": []
        }
        
        try:
            # Test frontend accessibility
            try:
                frontend_response = self.session.get(self.frontend_url, timeout=10)
                if frontend_response.status_code == 200:
                    content = frontend_response.text
                    assessment["frontend_accessibility"] = {
                        "accessible": True,
                        "has_title": '<title>' in content,
                        "has_meta_viewport": 'viewport' in content,
                        "content_length": len(content)
                    }
                    
                    # Check for basic accessibility features
                    if not assessment["frontend_accessibility"]["has_title"]:
                        assessment["issues_found"].append({
                            "severity": "LOW",
                            "component": "Frontend UX",
                            "description": "Missing page title"
                        })
                        
                else:
                    assessment["frontend_accessibility"] = {"accessible": False}
                    assessment["issues_found"].append({
                        "severity": "HIGH", 
                        "component": "Frontend UX",
                        "description": f"Frontend not accessible: {frontend_response.status_code}"
                    })
                    
            except Exception as e:
                assessment["frontend_accessibility"] = {"accessible": False}
                assessment["issues_found"].append({
                    "severity": "HIGH",
                    "component": "Frontend UX",
                    "description": f"Frontend accessibility test failed: {str(e)}"
                })
            
            # Test documentation quality
            try:
                docs_response = self.session.get(f"{self.base_url}/docs")
                if docs_response.status_code == 200:
                    docs_content = docs_response.text
                    assessment["documentation_quality"] = {
                        "available": True,
                        "has_swagger": 'swagger' in docs_content.lower() or 'openapi' in docs_content.lower(),
                        "comprehensive": len(docs_content) > 10000  # Basic heuristic
                    }
                else:
                    assessment["documentation_quality"] = {"available": False}
                    assessment["issues_found"].append({
                        "severity": "MEDIUM",
                        "component": "Documentation",
                        "description": "API documentation not accessible"
                    })
                    
            except Exception as e:
                assessment["documentation_quality"] = {"available": False}
                assessment["issues_found"].append({
                    "severity": "MEDIUM",
                    "component": "Documentation",
                    "description": f"Documentation test failed: {str(e)}"
                })
            
            # Calculate UX score
            ux_factors = []
            
            # Frontend accessibility factor
            if assessment["frontend_accessibility"].get("accessible"):
                frontend_score = 100
                if not assessment["frontend_accessibility"].get("has_title"):
                    frontend_score -= 10
                if not assessment["frontend_accessibility"].get("has_meta_viewport"):
                    frontend_score -= 10
                ux_factors.append(frontend_score)
            else:
                ux_factors.append(0)
            
            # Documentation factor
            if assessment["documentation_quality"].get("available"):
                docs_score = 80
                if assessment["documentation_quality"].get("has_swagger"):
                    docs_score += 20
                ux_factors.append(docs_score)
            else:
                ux_factors.append(0)
            
            assessment["overall_score"] = sum(ux_factors) / len(ux_factors) if ux_factors else 0
            
        except Exception as e:
            assessment["issues_found"].append({
                "severity": "HIGH",
                "component": "UX Assessment",
                "description": f"User experience assessment failed: {str(e)}"
            })
            assessment["overall_score"] = 0
        
        return assessment
    
    def _assess_system_completeness(self) -> Dict[str, Any]:
        """Assess overall system completeness"""
        logger.info("üìã Assessing System Completeness...")
        
        # Expected features for a complete AI Model Validation Platform
        expected_features = {
            "video_management": {
                "upload": True,  # Available
                "listing": True,  # Available  
                "individual_retrieval": False,  # Missing - identified issue
                "processing": True,  # Available
                "metadata_extraction": True  # Available
            },
            "project_management": {
                "creation": True,  # Available
                "listing": True,   # Available
                "retrieval": True, # Available
                "update": True,    # Available
                "deletion": True   # Available
            },
            "annotation_system": {
                "video_annotation": True,  # Available (endpoints exist)
                "annotation_sessions": True,  # Available
                "annotation_export": True     # Available
            },
            "test_execution": {
                "test_sessions": True,      # Available
                "detection_events": True,   # Available
                "validation_workflow": True # Available
            },
            "real_time_features": {
                "websocket_support": True,  # Available
                "progress_tracking": True   # Available
            },
            "api_features": {
                "documentation": True,      # Available
                "error_handling": True,     # Available
                "health_monitoring": True   # Available
            }
        }
        
        # Calculate completeness scores
        completeness_scores = {}
        missing_features = []
        
        for category, features in expected_features.items():
            implemented_features = sum(1 for feature, implemented in features.items() if implemented)
            total_features = len(features)
            completeness_scores[category] = (implemented_features / total_features) * 100
            
            # Track missing features
            for feature, implemented in features.items():
                if not implemented:
                    missing_features.append(f"{category}.{feature}")
        
        overall_completeness = sum(completeness_scores.values()) / len(completeness_scores)
        
        return {
            "overall_completeness": overall_completeness,
            "category_completeness": completeness_scores,
            "missing_features": missing_features,
            "total_expected_features": sum(len(features) for features in expected_features.values()),
            "implemented_features": sum(len(features) for features in expected_features.values()) - len(missing_features)
        }
    
    def _calculate_production_readiness(self, report: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate overall production readiness score and recommendation"""
        
        # Weight different assessment categories
        weights = {
            "critical_functionality": 0.40,  # 40% weight - most important
            "performance": 0.25,             # 25% weight
            "error_handling": 0.20,          # 20% weight  
            "user_experience": 0.15          # 15% weight
        }
        
        # Extract scores from assessments
        scores = {
            "critical_functionality": report["critical_functionality_assessment"]["overall_score"],
            "performance": report["performance_assessment"]["overall_score"],
            "error_handling": report["error_handling_assessment"]["overall_score"],
            "user_experience": report["user_experience_assessment"]["overall_score"]
        }
        
        # Calculate weighted overall score
        overall_score = sum(scores[category] * weights[category] for category in weights.keys())
        
        # Determine recommendation based on score and critical issues
        critical_issues = []
        all_issues = []
        
        for assessment_name, assessment in report.items():
            if isinstance(assessment, dict) and "issues_found" in assessment:
                for issue in assessment["issues_found"]:
                    all_issues.append(issue)
                    if issue.get("severity") == "HIGH":
                        critical_issues.append(issue)
        
        # Production readiness determination
        if overall_score >= 90 and len(critical_issues) == 0:
            recommendation = "APPROVED FOR PRODUCTION"
            risk_level = "LOW"
            readiness_status = "PRODUCTION_READY"
        elif overall_score >= 80 and len(critical_issues) == 0:
            recommendation = "APPROVED FOR STAGING"
            risk_level = "MEDIUM"
            readiness_status = "STAGING_READY"
        elif overall_score >= 70 and len(critical_issues) <= 1:
            recommendation = "REQUIRES MINOR FIXES"
            risk_level = "MEDIUM"
            readiness_status = "NEEDS_MINOR_FIXES"
        else:
            recommendation = "REQUIRES SIGNIFICANT FIXES"
            risk_level = "HIGH"
            readiness_status = "NEEDS_MAJOR_FIXES"
        
        return {
            "overall_score": overall_score,
            "category_scores": scores,
            "weights_used": weights,
            "recommendation": recommendation,
            "risk_level": risk_level,
            "readiness_status": readiness_status,
            "critical_issues_count": len(critical_issues),
            "total_issues_count": len(all_issues),
            "system_completeness": report["system_completeness"]["overall_completeness"]
        }
    
    def _generate_executive_summary(self, report: Dict[str, Any]) -> Dict[str, Any]:
        """Generate executive summary"""
        
        production_readiness = report["production_readiness"]
        
        # Key metrics
        key_metrics = {
            "overall_system_score": production_readiness["overall_score"],
            "system_completeness": production_readiness["system_completeness"],
            "critical_issues": production_readiness["critical_issues_count"],
            "total_issues": production_readiness["total_issues_count"],
            "recommendation": production_readiness["recommendation"],
            "risk_level": production_readiness["risk_level"]
        }
        
        # Success highlights
        success_highlights = []
        
        if report["critical_functionality_assessment"]["overall_score"] >= 80:
            success_highlights.append("‚úÖ Core functionality working well")
        
        if report["performance_assessment"]["overall_score"] >= 80:
            success_highlights.append("‚úÖ Good performance characteristics")
            
        if report["error_handling_assessment"]["overall_score"] >= 80:
            success_highlights.append("‚úÖ Robust error handling")
            
        if report["user_experience_assessment"]["overall_score"] >= 80:
            success_highlights.append("‚úÖ Acceptable user experience")
        
        # Key concerns
        key_concerns = []
        
        if production_readiness["critical_issues_count"] > 0:
            key_concerns.append(f"‚ö†Ô∏è {production_readiness['critical_issues_count']} critical issues need resolution")
        
        if report["critical_functionality_assessment"]["overall_score"] < 80:
            key_concerns.append("‚ö†Ô∏è Core functionality has significant gaps")
            
        if report["performance_assessment"]["overall_score"] < 70:
            key_concerns.append("‚ö†Ô∏è Performance issues detected")
        
        # System readiness summary
        if production_readiness["overall_score"] >= 85:
            system_readiness = "System demonstrates high quality and is suitable for deployment with minor considerations"
        elif production_readiness["overall_score"] >= 75:
            system_readiness = "System is functional with good quality, suitable for staging environment"
        elif production_readiness["overall_score"] >= 65:
            system_readiness = "System has basic functionality but requires improvements before production deployment"
        else:
            system_readiness = "System requires significant improvements to meet production standards"
        
        return {
            "key_metrics": key_metrics,
            "success_highlights": success_highlights,
            "key_concerns": key_concerns,
            "system_readiness_summary": system_readiness,
            "executive_recommendation": f"{production_readiness['recommendation']} (Score: {production_readiness['overall_score']:.1f}%)"
        }
    
    def _generate_recommendations(self, report: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate specific recommendations based on assessment"""
        
        recommendations = []
        
        # Critical functionality recommendations
        crit_func = report["critical_functionality_assessment"]
        if crit_func["overall_score"] < 90:
            for issue in crit_func["issues_found"]:
                if issue["severity"] == "HIGH":
                    recommendations.append({
                        "priority": "HIGH",
                        "category": "Critical Functionality",
                        "issue": issue["description"],
                        "recommendation": f"Fix {issue['component']} issue: {issue['description']}",
                        "impact": "Blocks production deployment"
                    })
                elif issue["severity"] == "MEDIUM":
                    recommendations.append({
                        "priority": "MEDIUM",
                        "category": "Feature Completeness", 
                        "issue": issue["description"],
                        "recommendation": f"Implement missing {issue['component']} feature",
                        "impact": "Improves system completeness and user experience"
                    })
        
        # Performance recommendations
        performance = report["performance_assessment"]
        if performance["overall_score"] < 85:
            for issue in performance["issues_found"]:
                recommendations.append({
                    "priority": "MEDIUM",
                    "category": "Performance",
                    "issue": issue["description"],
                    "recommendation": f"Optimize {issue['component']} performance",
                    "impact": "Improves user experience and system scalability"
                })
        
        # System completeness recommendations
        completeness = report["system_completeness"]
        if completeness["missing_features"]:
            recommendations.append({
                "priority": "MEDIUM",
                "category": "System Completeness",
                "issue": f"{len(completeness['missing_features'])} features missing from complete implementation",
                "recommendation": f"Consider implementing missing features: {', '.join(completeness['missing_features'])}",
                "impact": "Increases system completeness and functionality"
            })
        
        # User experience recommendations
        ux = report["user_experience_assessment"]
        if ux["overall_score"] < 80:
            for issue in ux["issues_found"]:
                recommendations.append({
                    "priority": "LOW" if issue["severity"] == "LOW" else "MEDIUM",
                    "category": "User Experience",
                    "issue": issue["description"],
                    "recommendation": f"Improve {issue['component']}",
                    "impact": "Enhances user satisfaction and adoption"
                })
        
        # General recommendations based on overall score
        overall_score = report["production_readiness"]["overall_score"]
        if overall_score < 90:
            recommendations.append({
                "priority": "HIGH" if overall_score < 80 else "MEDIUM",
                "category": "General",
                "issue": "System score below production threshold",
                "recommendation": "Address high-priority issues and improve overall system quality",
                "impact": "Meets production deployment standards"
            })
        
        # Sort recommendations by priority
        priority_order = {"HIGH": 0, "MEDIUM": 1, "LOW": 2}
        recommendations.sort(key=lambda x: priority_order.get(x["priority"], 3))
        
        return recommendations

def main():
    """Generate and display final comprehensive validation report"""
    
    print("üéØ AI MODEL VALIDATION PLATFORM - FINAL COMPREHENSIVE VALIDATION REPORT")
    print("=" * 90)
    
    validator = FinalValidationReport()
    
    try:
        report = validator.generate_comprehensive_report()
        
        # Display executive summary
        print("\nüìä EXECUTIVE SUMMARY")
        print("-" * 50)
        executive = report["executive_summary"]
        
        print(f"Overall System Score: {executive['key_metrics']['overall_system_score']:.1f}%")
        print(f"System Completeness: {executive['key_metrics']['system_completeness']:.1f}%")
        print(f"Critical Issues: {executive['key_metrics']['critical_issues']}")
        print(f"Total Issues: {executive['key_metrics']['total_issues']}")
        print(f"Risk Level: {executive['key_metrics']['risk_level']}")
        print(f"\nRecommendation: {executive['executive_recommendation']}")
        
        print(f"\n{executive['system_readiness_summary']}")
        
        # Display success highlights
        if executive["success_highlights"]:
            print(f"\nüéâ SUCCESS HIGHLIGHTS:")
            for highlight in executive["success_highlights"]:
                print(f"  {highlight}")
        
        # Display key concerns
        if executive["key_concerns"]:
            print(f"\n‚ö†Ô∏è  KEY CONCERNS:")
            for concern in executive["key_concerns"]:
                print(f"  {concern}")
        
        # Display category scores
        print("\nüìà CATEGORY SCORES:")
        print("-" * 30)
        production = report["production_readiness"]
        for category, score in production["category_scores"].items():
            print(f"  {category.replace('_', ' ').title()}: {score:.1f}%")
        
        # Display top recommendations
        recommendations = report["recommendations"]
        if recommendations:
            print(f"\nüîß TOP RECOMMENDATIONS:")
            print("-" * 40)
            for i, rec in enumerate(recommendations[:5]):  # Show top 5
                print(f"  {i+1}. [{rec['priority']}] {rec['recommendation']}")
                print(f"     Impact: {rec['impact']}")
        
        # Final determination
        print(f"\nüèÜ FINAL DETERMINATION:")
        print("-" * 40)
        readiness = production["readiness_status"]
        
        if readiness == "PRODUCTION_READY":
            print("‚úÖ SYSTEM IS READY FOR PRODUCTION DEPLOYMENT")
        elif readiness == "STAGING_READY":
            print("‚ö†Ô∏è  SYSTEM IS READY FOR STAGING DEPLOYMENT")
        elif readiness == "NEEDS_MINOR_FIXES":
            print("üîß SYSTEM REQUIRES MINOR FIXES BEFORE DEPLOYMENT")
        else:
            print("‚ùå SYSTEM REQUIRES SIGNIFICANT FIXES BEFORE DEPLOYMENT")
        
        # Save detailed report
        report_filename = f"final_comprehensive_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_filename, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        print(f"\nüìÑ Detailed report saved to: {report_filename}")
        
        return report
        
    except Exception as e:
        print(f"‚ùå CRITICAL ERROR generating validation report: {str(e)}")
        return None

if __name__ == "__main__":
    main()